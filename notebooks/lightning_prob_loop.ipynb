{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71dce86-478f-4a75-9a33-e3c825f92d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial  # for pd.Series.map(ztfs)\n",
    "from multiprocessing.pool import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ahijevyc import G211, lightning\n",
    "from ml_functions import brier_skill_score, get_args, get_flash_pred\n",
    "from ahijevyc.spc import get_issuance_time\n",
    "from statisticplot import ax_features, stat_plots\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format=\"%(asctime)s %(message)s\", force=True)\n",
    "tmpdir = Path(os.getenv(\"TMPDIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c7356-0f8d-44d4-a3bc-9ef1e923563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7143d-3e5e-4f93-b745-9fc908310f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map projection\n",
    "map_crs = G211.G211\n",
    "g211 = G211.GridManager()\n",
    "grid = g211.grid\n",
    "grid = grid.to_crs(ccrs.PlateCarree())\n",
    "dpi = 200\n",
    "conus_mask = g211.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465c7f7-112d-4b8c-9688-a0e9694f889b",
   "metadata": {},
   "source": [
    "### Load SPC Tstm Outlook\n",
    "downloaded by [get_enhtstm.ipynb](get_enhtstm.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172dbdef-1e7a-4b32-be4a-25b3ac6c9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfiles = (tmpdir / \"enhtstm\").glob(\"????/*.zip\")\n",
    "sfiles = sorted(sfiles)\n",
    "column = \"DN\"\n",
    "sfiles[0].name, sfiles[-1].name, len(sfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af82a5-6a60-4009-897a-015aa8ae47be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obsvar = \"cg\"\n",
    "o_thresh = 1\n",
    "twin = 4\n",
    "rptdist = 20\n",
    "clobber = False\n",
    "\n",
    "platform = \"glm\" if obsvar == \"flashes\" else \"wbug\"\n",
    "\n",
    "f = f\"{obsvar}_{rptdist}km_{twin}hr\"\n",
    "# Define observation/probability thresholds\n",
    "thresh = pd.Series([o_thresh], name=f\"{obsvar} threshold\")\n",
    "pthreshSPC = pd.Series([0, 10, 40, 70], name=\"fcst prob\\nthresh\") / 100.0\n",
    "args = get_args(o_thresh, twin)\n",
    "args.teststart = pd.to_datetime(\"20210101\")\n",
    "args.testend = pd.to_datetime(\"20220101\")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd9f44-6361-4479-9daa-9c3ad50c6bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spc_forecast_obs(sfile: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sfile:str\n",
    "        a path like\n",
    "        '/glade/derecho/scratch/ahijevyc/tmp/enhtstm/2020/enh00_20200105_052000_202001051206-shp.zip'\n",
    "        from which you can derive issuance and valid time range\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame with forecast and observations from same period\n",
    "    \"\"\"\n",
    "\n",
    "    assert twin > 1, (\n",
    "        \"no NNPF time windows for twin == 1 that conform to SPC time windows. \"\n",
    "        \"NNPF twin=1 start and end on the half-hour, while SPC time windows \"\n",
    "        \"start and end on the hour. But that is okay. You can still plot \"\n",
    "        \"NNPF performance at all valid times, regardless of SPC fcst below.\"\n",
    "    )\n",
    "    record = os.path.basename(sfile)\n",
    "\n",
    "    # If there are multiple datasets in a folder in the ZIP file, you also have to specify the filename:\n",
    "    filename = record.rstrip('-shp.zip') + '.shp'\n",
    "    ifile = f\"{sfile}!{filename}\"\n",
    "\n",
    "    f = geopandas.read_file(ifile).to_crs(ccrs.PlateCarree())\n",
    "\n",
    "    # Get forecast valid start and end from sfile string.\n",
    "    issue, valid_start, valid_end = get_issuance_time(record)\n",
    "    logging.info(f\"{issue} {valid_start} {valid_end}\")\n",
    "\n",
    "    # Get observations from DataFrame from ml_functions.load_df()\n",
    "    # instead of lightning.get_obs()\n",
    "    # so you have the same data, and don't \"repeat\" the same\n",
    "    # function with different code.\n",
    "    # get_obs allows GLM or ENTLN to be missing for a time\n",
    "    # but load_df does not.\n",
    "\n",
    "    # Uses `obs` DataFrame already sliced from Y DataFrame outside function.\n",
    "    # I don't include it as an input parameter because this is run in a Pool.\n",
    "    # I think it would make a copy of obs for each worker\n",
    "    # if I included it as an input parameter.\n",
    "\n",
    "    valid_times = pd.date_range(\n",
    "        start=valid_start + pd.Timedelta(hours=twin / 2),\n",
    "        end=valid_end - pd.Timedelta(hours=twin / 2),\n",
    "        freq=f\"{twin}h\",\n",
    "    )\n",
    "    # if all expected valid_times are available\n",
    "    logging.info(f\"{len(valid_times)} valid_times {valid_times}\")\n",
    "    if all(\n",
    "        [valid_time in obs.index.get_level_values(\"valid_time\") for valid_time in valid_times]\n",
    "    ):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "            # Reduce initialization_time dimension for multiple\n",
    "            # initialization_times that have the same valid_time.\n",
    "            # The validation data, obs, are the same for each init time so just\n",
    "            # take the first.\n",
    "            # You don't what what position \"valid_time\" level is in. It could be\n",
    "            # 2nd position, or 4th, or whatever.\n",
    "            ix = obs.index.get_level_values(\"valid_time\").isin(valid_times)\n",
    "            o = (obs.loc[ix, args.labels]\n",
    "                #obs.loc[\n",
    "                #    (slice(None), slice(None), slice(None), valid_times),\n",
    "                #    args.labels,\n",
    "                #]\n",
    "                .groupby([\"x\", \"y\", \"valid_time\"])\n",
    "                .first()\n",
    "            )\n",
    "        o = o.groupby(\n",
    "            [\"x\", \"y\"]\n",
    "        ).sum()  # sum flashes for each (x,y) along valid_times dimension\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Assign value to conus grid points within polygons\n",
    "    f = grid[conus_mask.values.ravel()].sjoin(f, how=\"left\")\n",
    "\n",
    "    f[\"record\"] = record\n",
    "    f[\"issue\"] = issue\n",
    "    # convert to str avoids TypeError: Object of type Timestamp is not JSON serializable\n",
    "    # when saving to parquet\n",
    "    f[\"valid_start\"] = valid_start.strftime(\"%Y%m%d %H:%M\")\n",
    "    f[\"valid_end\"] = valid_end.strftime(\"%Y%m%d %H:%M\")\n",
    "    columns = [\"record\", \"DN\", \"issue\", \"valid_start\", \"valid_end\"]\n",
    "    # fill missing fcst prob (outside any polygon) with 0. Join with obs.\n",
    "    f = f[columns].fillna(0).join(o)\n",
    "    f = f.set_index(\"record\", append=True)\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1fa75-6ec6-479c-83c6-cd0c162d3546",
   "metadata": {},
   "source": [
    "### Load DataFrame of NNPF forecasts / observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6424f-b2b3-4c13-9828-c7c3d573ff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get observations from NNPF forecast/obs DataFrame\n",
    "Y = get_flash_pred(args, clobber=clobber)\n",
    "\n",
    "# ensmean of fits and folds\n",
    "# Don't be clever by subtracting {\"fit\", \"fold\"} set from Y.index.names\n",
    "# Order of returned list of levels was random. valid_time did not always come 2nd.\n",
    "levels = [\"initialization_time\", \"valid_time\", \"y\", \"x\", \"forecast_hour\", \"lat\", \"lon\"]\n",
    "logging.warning(f\"groupby {levels} for ensmean\")\n",
    "ensmean = Y.groupby(level=levels).mean()\n",
    "\n",
    "# obs used in spc_forecast_obs() defined above\n",
    "logging.warning(\"obs (label) slice\")\n",
    "obs = ensmean.xs(\"y_label\", axis=\"columns\", level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea63997-8686-4044-91a2-697e1061f8f1",
   "metadata": {},
   "source": [
    "## Verify SPC Thunderstorm Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c7b86-bcb5-4019-936c-4bfe8dcc7eca",
   "metadata": {},
   "source": [
    "### Match obs to each SPC forecast in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030fd95-27ad-4857-810f-6d3d2cf64e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffile = (\n",
    "    tmpdir\n",
    "    / f\"spc_fcst.{args.teststart.strftime('%Y%m%d%H')}-{args.testend.strftime('%Y%m%d%H')}.{twin}hr.par\"\n",
    ")\n",
    "\n",
    "if not clobber and os.path.exists(ffile):  # and os.path.exists(ofile):\n",
    "    logging.warning(f\"read {ffile} {os.path.getsize(ffile)/1e6:.1f}M \")\n",
    "    spc_fcst = pd.read_parquet(ffile)\n",
    "else:\n",
    "    logging.warning(f\"make new file {ffile}\")\n",
    "    if twin == 2:\n",
    "        logging.warning(\"Sure u want to compare NNPFs with 2-hour time window to SPC?\")\n",
    "\n",
    "    assert obs[f].sum() > 0, (\n",
    "        \"no lightning obs. did you run cells further down that changed\"\n",
    "        \" obs and then come back\"\n",
    "        \" to this cell?\"\n",
    "    )\n",
    "\n",
    "    # Use multiple cpus (like 16) and 60+GB memory.\n",
    "    # 16 cpu, 89 GB, 16 processes took 17 minutes (twin=4)\n",
    "    with Pool(processes=16) as pool:\n",
    "        result = pool.map(\n",
    "            spc_forecast_obs,\n",
    "            tqdm(sfiles),\n",
    "        )\n",
    "\n",
    "    spc_fcst = pd.concat([x for x in result if x is not None])\n",
    "    spc_fcst.to_parquet(ffile)\n",
    "\n",
    "# tried saving this to parquet ofile but tuple in column broke arrow engine\n",
    "spc_fcst[\"prodid\"] = list(\n",
    "    zip(\n",
    "        spc_fcst.issue,\n",
    "        pd.to_datetime(spc_fcst.valid_start).dt.hour,\n",
    "        pd.to_datetime(spc_fcst.valid_end).dt.hour,\n",
    "    )\n",
    ")\n",
    "\n",
    "logging.info(\"trim spc_fcst to testing range of neural network\")\n",
    "print(spc_fcst.valid_start.min())\n",
    "print(spc_fcst.valid_start.max())\n",
    "before_trim = len(spc_fcst)\n",
    "spc_fcst = spc_fcst[\n",
    "    (pd.to_datetime(spc_fcst.valid_start) >= pd.to_datetime(args.teststart))\n",
    "    & (pd.to_datetime(spc_fcst.valid_end) <= pd.to_datetime(args.testend))\n",
    "]\n",
    "logging.warning(\n",
    "    f\"kept {len(spc_fcst)}/{before_trim} ({len(spc_fcst)/before_trim:%}) spc fcsts\"\n",
    ")\n",
    "\n",
    "if args.teststart < pd.to_datetime(\"20220101\"):\n",
    "    assert \"enh00_20211231_312000_202112310559-shp.zip\" in spc_fcst.index.get_level_values(\n",
    "        \"record\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bee4be-1bf3-4321-bf77-cb33d3616404",
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0862412-6c41-40eb-8a79-7eea7c00b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = spc_fcst.copy()\n",
    "rows[\"timestamp\"] = pd.to_datetime(rows.index.get_level_values(\"record\").str[22:34], format='%Y%m%d%H%M')\n",
    "rows[\"leadtime\"] = rows[\"timestamp\"] - pd.to_datetime(rows[\"valid_start\"])\n",
    "rows[\"leadtime\"] = rows[\"leadtime\"].dt.total_seconds()/3600\n",
    "rows.hist(\"leadtime\", by=\"prodid\", figsize=(12,10))\n",
    "ofile = tmpdir / f\"hist.{args.teststart.strftime('%Y%m%d%H')}-{args.testend.strftime('%Y%m%d%H')}.before.png\"\n",
    "plt.gcf().suptitle(\"before removing old duplicate products\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca89fa9-1e5b-4ad7-9f65-70588278dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.iloc[rows[\"leadtime\"].argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d4cda-8a8b-446c-9049-d958eaac5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cnt(cnt, **kwargs):\n",
    "    df = pd.DataFrame(\n",
    "        cnt.index.tolist(),\n",
    "        columns=[\"issuance UTC\", \"valid_start_hour\", \"valid_end_hour\"],\n",
    "    )\n",
    "    df[\"valid range hours UTC\"] = list(zip(df.valid_start_hour, df.valid_end_hour))\n",
    "    df[\"n\"] = cnt.values\n",
    "    df = df.reset_index().pivot(\n",
    "        columns=\"valid range hours UTC\", index=\"issuance UTC\", values=\"n\"\n",
    "    )\n",
    "    df = df.reindex(\n",
    "        [\"0130\", \"2100\", \"1700\", \"1300\", \"0600\"],\n",
    "        columns=[(12, 16), (16, 20), (20, 0), (0, 4), (4, 12)],\n",
    "    )\n",
    "    g = sns.heatmap(df, annot=True, cbar=False, **kwargs)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78b0bc-da2f-4075-ae20-f23e86b9c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldcnt = spc_fcst.groupby(\"prodid\").count().iloc[:,0]/1308\n",
    "cnt = spc_fcst.groupby([\"prodid\",\"valid_start\"]).last().groupby(\"prodid\").count().iloc[:,0]\n",
    "plot_cnt(cnt-oldcnt, fmt=\".0f\", cmap=\"Reds_r\")\n",
    "ofile = tmpdir / f\"cnt_change_after_removing_extra.{args.teststart.strftime('%Y%m%d%H')}-{args.testend.strftime('%Y%m%d%H')}.png\"\n",
    "plt.gca().set_title(f\"(a) Change after removing extra outlooks in {args.teststart.strftime('%Y')}\", loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a4d5a-fcde-4ad5-898b-a361c6d55af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5.5,4.5))\n",
    "plot_cnt(cnt, ax=ax, fmt=\".0f\", cmap=\"Blues\")\n",
    "ofile = tmpdir / f\"cnt.{args.teststart.strftime('%Y%m%d%H')}-{args.testend.strftime('%Y%m%d%H')}.png\"\n",
    "plt.gca().set_title(f\"(a) Number of SPC Thunderstorm Outlooks during {args.teststart.strftime('%Y')}\", loc=\"left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4af94-3549-4271-a3b5-c939721c72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_fcst.groupby([\"y\",\"x\",\"prodid\",\"valid_start\"]).last().groupby([\"prodid\",\"valid_start\"]).count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc616823-6a94-40d8-86fa-f617e3ecf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows2 = rows.reset_index(\"record\").sort_values(\"leadtime\").groupby([\"y\",\"x\",\"prodid\",\"valid_start\"]).last()\n",
    "rows2.hist(\"leadtime\", by=\"prodid\", figsize=(12,10))\n",
    "ofile = tmpdir / f\"hist.{args.teststart.strftime('%Y%m%d%H')}-{args.testend.strftime('%Y%m%d%H')}.after.png\"\n",
    "plt.gcf().suptitle(\"after removing old duplicate products\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d6977-547c-4713-8d88-dd709d8eda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_fcst = rows2.reset_index(level=[\"prodid\",\"valid_start\"]).set_index(\"record\", append=True)\n",
    "spc_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4619eaa-42a8-4987-82b1-5c588f863361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_hour_str(prodids):\n",
    "    valid_hour_range = prodids[0][1:]  # last 2 elements of first prodids tuple\n",
    "    same_valid_hour_range = len(set([x[1:] for x in prodids])) == 1\n",
    "    if same_valid_hour_range:\n",
    "        return \" \" + str(valid_hour_range)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Aggregate by valid time\n",
    "prodidss = [\n",
    "    [(\"0600\", 12, 16)],\n",
    "    [(\"0600\", 16, 20), (\"1300\", 16, 20)],\n",
    "    [(\"0600\", 20, 0), (\"1300\", 20, 0), (\"1700\", 20, 0)],\n",
    "    [(\"1300\", 0, 4), (\"1700\", 0, 4), (\"2100\", 0, 4)],\n",
    "    [(\"1700\", 4, 12), (\"2100\", 4, 12), (\"0130\", 4, 12)],\n",
    "]\n",
    "\n",
    "# Flatten list of lists to get every issuance and valid time in its own list\n",
    "prodidss = [[x] for prodid in prodidss for x in prodid]\n",
    "# All together\n",
    "prodidss = [[x for prodid in prodidss for x in prodid]]\n",
    "\n",
    "ncols, nrows = 2, 2\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 4, nrows * 4))\n",
    "title = \"\"\n",
    "\n",
    "\n",
    "spc_bss = {}\n",
    "for prodids in prodidss:\n",
    "    logging.info(f\"forecast products for {len(prodids)} prodids\")\n",
    "    ihr = spc_fcst.prodid.isin(prodids)\n",
    "    assert ihr.sum()\n",
    "    #print(ihr.sum() / G211.mask.values.sum())\n",
    "\n",
    "    # 1308 CONUS points per forecast\n",
    "    cnt = (spc_fcst[ihr].groupby(\"prodid\").size() /G211.mask.values.sum()).astype(int)\n",
    "\n",
    "    print(cnt.index.values, cnt.values, end=\" \")\n",
    "\n",
    "    title += \"  \".join([f\"{c}{v}\" for c, v in list(zip(cnt, cnt.index))])\n",
    "\n",
    "    thisf = spc_fcst.loc[ihr, \"DN\"] / 100\n",
    "    thisf.name = \"SPC\" + valid_hour_str(prodids)\n",
    "\n",
    "    fig = stat_plots(\n",
    "        spc_fcst.loc[ihr, f],\n",
    "        thisf,\n",
    "        thresh=thresh,\n",
    "        pthresh=pthreshSPC,\n",
    "        o_thresh_roc=o_thresh,\n",
    "        sep=0.01,\n",
    "        n_bins=11,\n",
    "        fig=fig,\n",
    "        no_perfect_line=True,\n",
    "    )\n",
    "    if len(prodids) == 1:\n",
    "        # Remember this for later\n",
    "        spc_bss[prodids[0]] = brier_skill_score(spc_fcst.loc[ihr,f]>=o_thresh, thisf)\n",
    "\n",
    "axes[0,0].set_xlabel(\"forecast probability\") # neat for paper\n",
    "axes[0,0].set_ylabel(\"observed relative frequency\") # neat for paper\n",
    "axes[0,0].plot([0, 0.1, 0.4, 0.7 ,1], [0.1, 0.4, 0.7, 1, 1], color=\"r\", alpha=0.5)\n",
    "axes[0,0].plot([0, 1], [0, 1], color=\"r\", alpha=0.5)\n",
    "\n",
    "title = f\"{o_thresh}+{f} {rptdist}km\"\n",
    "title += f\"  valid [{spc_fcst.valid_start.min()}, {spc_fcst.valid_end.max()}]\"\n",
    "plt.suptitle(title, wrap=True)\n",
    "plt.tight_layout()\n",
    "ofile = tmpdir / f\"enhtstm.{o_thresh:03d}+{f}_{rptdist}km.png\"\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce10e4-1ded-4e14-bb44-5d8c75d1e565",
   "metadata": {},
   "source": [
    "## Verify neural network probability forecast NNPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d24a87-a7c9-4595-b782-f3a0d9d8fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensmean.index.get_level_values(\"forecast_hour\").unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78938da8-234e-4d53-89a4-4fddd246a819",
   "metadata": {},
   "source": [
    "## Get obsvar at rptdist and twin from NNPF obs and fcst\n",
    "### Plot NNPF performance at all valid times, regardless of SPC fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f3de6-7c4e-4deb-8e7d-9d4dd6da8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def statjob(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    logging.info(f\"statjob: {group.name}\")\n",
    "    # seperate y_pred and labels and drop level 0\n",
    "    y_pred = group.xs(\"y_pred\", axis=\"columns\", level=0)\n",
    "    # labels went from bool to object dtype, so fix it or roc_auc_score will not recognize format\n",
    "    labels = group.xs(\"y_label\", axis=\"columns\", level=0).astype(bool)\n",
    "\n",
    "    bss = brier_skill_score(labels, y_pred)\n",
    "    base_rate = labels.mean()\n",
    "    # Default value is np.nan\n",
    "    # Don't assign Series to auc and aps on same line or they will remain equal even if you change one\n",
    "    auc = pd.Series(np.nan, index=labels.columns)\n",
    "    aps = pd.Series(np.nan, index=labels.columns)\n",
    "    # auc and aps require 2 unique labels, i.e. both True and False\n",
    "    two = labels.nunique() == 2\n",
    "    if two.any():\n",
    "        # average=None returns a metric for each label instead of one group average of all labels\n",
    "        auc[two] = sklearn.metrics.roc_auc_score(\n",
    "            labels.loc[:, two], y_pred.loc[:, two], average=None\n",
    "        )\n",
    "        aps[two] = sklearn.metrics.average_precision_score(\n",
    "            labels.loc[:, two], y_pred.loc[:, two], average=None\n",
    "        )\n",
    "    n = y_pred.count()\n",
    "    # geometry for geopandas dataframe\n",
    "    forecast_hour, lat_bin, lon_bin = group.name\n",
    "    poly = Polygon(\n",
    "        (\n",
    "            (lon_bin.left, lat_bin.left),\n",
    "            (lon_bin.left, lat_bin.right),\n",
    "            (lon_bin.right, lat_bin.right),\n",
    "            (lon_bin.right, lat_bin.left),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        dict(\n",
    "            bss=bss,\n",
    "            base_rate=base_rate,\n",
    "            auc=auc,\n",
    "            aps=aps,\n",
    "            n=n,\n",
    "            geometry=poly,\n",
    "            forecast_hour=0.5 * (forecast_hour.left + forecast_hour.right),\n",
    "        )\n",
    "    )\n",
    "    out.index.name = \"class\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47cc26-f6d6-4c24-9e44-543d2c111a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "fhr_bin = pd.cut(ensmean.index.get_level_values(\"forecast_hour\"), bins=range(0, 49, 6))\n",
    "lat_bin = pd.cut(ensmean.index.get_level_values(\"lat\"), bins=np.arange(25, 52.5, 2.5))\n",
    "lon_bin = pd.cut(ensmean.index.get_level_values(\"lon\"), bins=np.arange(-126, -64, 3))\n",
    "\n",
    "by_llfhr = ensmean.groupby(\n",
    "    by=[fhr_bin, lat_bin, lon_bin],\n",
    "    observed=True,\n",
    ").progress_apply(statjob)\n",
    "\n",
    "by_llfhr.index.names = (\"fhr_bin\", \"lat_bin\", \"lon_bin\", \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76919fef-833c-4d23-9764-b97557ec9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium.plugins\n",
    "\n",
    "m = folium.plugins.DualMap(location=[37, -95], zoom_start=3, layout=\"horizontal\")\n",
    "\n",
    "s = fhr_bin.categories[3]  # pick a forecast hour bin\n",
    "print(s)\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    by_llfhr.xs(f, level=\"class\").xs(s, level=\"fhr_bin\"), crs=ccrs.PlateCarree()\n",
    ")\n",
    "gdf.explore(\n",
    "    column=\"bss\",\n",
    "    vmin=0.0,\n",
    "    vmax=0.6,\n",
    "    style_kwds={\"fillOpacity\": 0.8},\n",
    "    cmap=\"Greens\",\n",
    "    m=m.m2,\n",
    ")\n",
    "gdf.explore(\n",
    "    column=\"base_rate\",\n",
    "    vmin=0,\n",
    "    vmax=0.02 * twin,\n",
    "    style_kwds={\"fillOpacity\": 0.8},\n",
    "    cmap=\"Purples\",\n",
    "    m=m.m1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0714c-3c33-47a4-b744-8600feafaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "%matplotlib inline\n",
    "extent = (-121, -72, 23, 50)\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6.6), subplot_kw={\"projection\": map_crs})\n",
    "\n",
    "legend_kwds = dict(orientation= \"horizontal\", shrink=0.9, aspect=25, pad=0.05 )\n",
    "\n",
    "for icat, row in zip([3,5], axes):\n",
    "    s = fhr_bin.categories[icat]  # pick a forecast hour bin\n",
    "    gdf = geopandas.GeoDataFrame(\n",
    "        by_llfhr.xs(f, level=\"class\").xs(s, level=\"fhr_bin\"), crs=ccrs.PlateCarree()\n",
    "    )\n",
    "    gdf[\"Base Rate (%)\"] = gdf[\"base_rate\"] * 100\n",
    "    ax = row[0]\n",
    "    title = f\"a) Afternoon ({s.left%24},{s.right%24}] UTC, fhr {s}\" \n",
    "    if icat == 5:\n",
    "        title = f\"b) Night ({s.left%24},{s.right%24}] UTC, fhr {s}\"  \n",
    "    ax.set_title(title, loc=\"left\")\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column=\"Base Rate (%)\",\n",
    "        vmin=0,\n",
    "        vmax=0.02 * twin,\n",
    "        alpha=0.8,\n",
    "        cmap=lightning.cmap,\n",
    "        norm = BoundaryNorm(np.arange(0, 12, 1), ncolors=lightning.cmap.N, clip=True),\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        legend=icat==5,\n",
    "        legend_kwds=legend_kwds | {\"label\":\"Base Rate (%)\"},\n",
    "    )\n",
    "    ax = row[1]\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column=\"bss\",\n",
    "        vmin=0.0,\n",
    "        vmax=0.6,\n",
    "        alpha=0.8,\n",
    "        cmap=\"Greens\",\n",
    "        norm = BoundaryNorm(np.arange(0, 0.55, 0.05), ncolors=256),\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        legend=icat==5,\n",
    "        legend_kwds=legend_kwds | {\"label\":\"Brier Skill Score\"},\n",
    "    )\n",
    "for ax in axes.ravel():\n",
    "    ax_features(ax)\n",
    "    ax.set_extent(extent)\n",
    "fig.tight_layout()\n",
    "ofile = tmpdir / \"spatial.png\"\n",
    "fig.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b11c7-5030-40ce-8dc1-2abab6a72ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat_bin = pd.cut(ensmean.index.get_level_values(\"lat\"), bins=1)\n",
    "lon_bin = pd.cut(ensmean.index.get_level_values(\"lon\"), bins=1)\n",
    "by_fhr = ensmean.groupby(\n",
    "    by=[fhr_bin, lat_bin, lon_bin],\n",
    "    observed=True,\n",
    ").progress_apply(statjob)\n",
    "\n",
    "by_fhr.index.names = (\"fhr_bin\", \"lat_bin\", \"lon_bin\", \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebcd9f-1975-48a4-8548-3a5bed966c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "# Figure dimensions\n",
    "# Don't overwrite fig variable from SPC stat_plot earlier.\n",
    "# You may want to overlay NNPF on SPC stat_plot.\n",
    "_ = plt.figure(figsize=(10, 7))\n",
    "\n",
    "topax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "botax = plt.subplot2grid((3, 1), (2, 0), rowspan=1, sharex=topax)\n",
    "botax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    by_fhr.xs(f, level=\"class\"),\n",
    "    x=\"forecast_hour\",\n",
    "    y=\"bss\",\n",
    "    ax=topax,\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax.set_ylim((0.08, 0.53))\n",
    "ax = sns.lineplot(\n",
    "    by_fhr.xs(f, level=\"class\"),\n",
    "    x=\"forecast_hour\",\n",
    "    y=\"base_rate\",\n",
    "    ax=botax,\n",
    "    marker=\"o\",\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291bdd9-f05f-4092-b822-9677e9757370",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "ncols, nrows = 2, 2\n",
    "gfig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 4, nrows * 4))\n",
    "for name, group in ensmean.groupby(\n",
    "    by=[fhr_bin, lat_bin, lon_bin],\n",
    "    observed=True,\n",
    "):\n",
    "    obs = group.xs(\"y_label\", axis=\"columns\", level=0)[f]\n",
    "    fcst = group.xs(\"y_pred\", axis=\"columns\", level=0)[f]\n",
    "    fcst.name = name[0]\n",
    "    if name[0].left >= 36:\n",
    "        continue\n",
    "    gfig = stat_plots(\n",
    "        obs,\n",
    "        fcst,\n",
    "        thresh=thresh,\n",
    "        pthresh=pd.Series(\n",
    "            np.round([0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9], 2), name=\"fcst prob\\nthresh\"\n",
    "        ),\n",
    "        o_thresh_roc=o_thresh,\n",
    "        sep=0.15,\n",
    "        fig=gfig,\n",
    "    )\n",
    "ofile = tmpdir / f\"nnpf.{o_thresh:03d}+{f}.by_fhr.png\"\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3c769-2e75-41df-9b11-8509589c0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ensmean.xs(\"y_label\", axis=\"columns\", level=0)\n",
    "fcst = ensmean.xs(\"y_pred\", axis=\"columns\", level=0)\n",
    "\n",
    "valid_range_str = (\n",
    "    f'[{ensmean.index.get_level_values(\"valid_time\").min()},'\n",
    "    f' {ensmean.index.get_level_values(\"valid_time\").max()}]'\n",
    ")\n",
    "\n",
    "_ = stat_plots(\n",
    "    obs[f],\n",
    "    fcst[f],\n",
    "    thresh=thresh,\n",
    "    pthresh=pd.Series(\n",
    "        np.round([0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9], 2), name=\"fcst prob\\nthresh\"\n",
    "    ),\n",
    "    o_thresh_roc=o_thresh,\n",
    "    sep=0.15,\n",
    "    suptitle=f\"n={len(ensmean)/G211.mask.values.sum()} {o_thresh}+{f} {levels}\\nvalid {valid_range_str}\",\n",
    ")\n",
    "ofile = tmpdir / f\"nnpf.{o_thresh:03d}+{f}.all.{'.'.join(levels)}.png\"\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daed91-774e-4247-b7c5-4ed480fc3423",
   "metadata": {},
   "source": [
    "## Restrict NNPF to valid times of SPC Thunderstorm Outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d96041-763d-4353-b13f-c4cc4d6b345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprod(record: str, fcst=fcst):\n",
    "    \"\"\"\n",
    "    Return NNPF for the same valid range as\n",
    "    an spc forecast record.\n",
    "    Valid range may span multiple time windows, each\n",
    "    with a separate NNPF.\n",
    "    \"\"\"\n",
    "    s = record.name\n",
    "    issue, valid_start, valid_end = get_issuance_time(s)\n",
    "    NNPFtimes = pd.date_range(\n",
    "        start=valid_start + pd.Timedelta(hours=twin / 2),\n",
    "        end=valid_end - pd.Timedelta(hours=twin / 2),\n",
    "        freq=f\"{twin}h\",\n",
    "    )\n",
    "    assert fcst.index.names[1] == \"valid_time\", \"valid_time must be MultiIndex level 1\"\n",
    "    try:\n",
    "        thisfcst = fcst.loc[(slice(None), NNPFtimes), :]\n",
    "        # Probability of no occurences during multiple time windows\n",
    "        # equals product of 1-p for each time window.\n",
    "        prob_none = (1 - thisfcst).groupby([\"y\", \"x\"]).prod()\n",
    "        # Probability of one or more occurrences is 1 minus that.\n",
    "        p = 1 - prob_none\n",
    "        return p\n",
    "    except KeyError as err:\n",
    "        logging.warning(f\"{err} {s} {issue} [{valid_start},{valid_end}]\")\n",
    "    except:\n",
    "        logging.error(f\"pprod broke on {s} {issue} [{valid_start},{valid_end}]\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "fhr_start = int(12 + twin / 2)\n",
    "fhr_end = int(36 - twin / 2)\n",
    "logging.warning(\n",
    "    f\"Limit forecast hour range [{fhr_start}, {fhr_end}], get ensemble mean\"\n",
    ")\n",
    "# selected forecast hours only. drops level by default\n",
    "fcst = fcst.xs(slice(fhr_start, fhr_end), level=\"forecast_hour\")\n",
    "\n",
    "pprod_par = tmpdir / f\"ps.{args.teststart}-{args.testend}.fhr{fhr_start}-{fhr_end}.{o_thresh:03d}+{platform}.par\"\n",
    "if not clobber and os.path.exists(pprod_par):\n",
    "    logging.warning(f\"use old NNPF probability product output {pprod_par}\")\n",
    "    p = pd.read_parquet(pprod_par)\n",
    "else:\n",
    "    time0 = time.time()\n",
    "    logging.warning(\n",
    "        f\"Save NNPF probability for each spc forecast (i.e. record) to {pprod_par}\"\n",
    "    )\n",
    "    tqdm.pandas()\n",
    "    p = spc_fcst.groupby(\"record\").progress_apply(pprod, fcst=fcst)\n",
    "    p.to_parquet(pprod_par)\n",
    "    print(time.time() - time0)\n",
    "\n",
    "logging.warning(\n",
    "    \"get (issuance time, valid hours) from NNPF forecasts matched to SPC records\"\n",
    ")\n",
    "x = [get_issuance_time(s) for s in tqdm(p.index.get_level_values(\"record\"))]\n",
    "issue, valid_start, valid_end = map(np.array, zip(*x))\n",
    "\n",
    "valid_start_hours = np.array([x.hour for x in valid_start])\n",
    "valid_end_hours = np.array([x.hour for x in valid_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509053c-2169-430d-8659-f9e93e710bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d52c3-e658-42a0-997d-301b8a738683",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "flat_prods = [\n",
    "    (\"0600\", 12, 16),\n",
    "    (\"0600\", 16, 20), (\"1300\", 16, 20),\n",
    "    (\"0600\", 20, 0), (\"1300\", 20, 0), (\"1700\", 20, 0),\n",
    "    (\"1300\", 0, 4), (\"1700\", 0, 4), (\"2100\", 0, 4),\n",
    "    (\"1700\", 4, 12), (\"2100\", 4, 12), (\"0130\", 4, 12),\n",
    "]\n",
    "leadtime_spc = {(i,v0,v1) : (v0 - (int(i[0:2])+float(i[2:])/60)) % 24 for (i, v0, v1) in flat_prods}\n",
    "leadtime_spc = pd.Series(leadtime_spc)\n",
    "leadtime_spc.index = leadtime_spc.index.to_flat_index()\n",
    "\n",
    "leadtime_NNPF = {(i,v0,v1) : ((v0-12) % 24)+12 for (i, v0, v1) in flat_prods}\n",
    "leadtime_NNPF = pd.Series(leadtime_NNPF)\n",
    "leadtime_NNPF.index = leadtime_NNPF.index.to_flat_index()\n",
    "\n",
    "# Don't use fig. save it from SPC stat_plot\n",
    "_, ax = plt.subplots(ncols=2, figsize=(9,4), sharey=True)\n",
    "g = plot_cnt(leadtime_spc, cmap=\"Oranges\", ax=ax[0], vmax=28)\n",
    "g.set_title(\"(b) SPC Thunderstorm Outlook\", loc=\"left\")\n",
    "g = plot_cnt(leadtime_NNPF, cmap=\"Oranges\", ax=ax[1], vmax=28)\n",
    "g.set_title(\"(c) 00 UTC CG1-NNPF\", loc=\"left\")\n",
    "plt.suptitle(\"Lead time in hours\")\n",
    "ofile = tmpdir / \"leadtime.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf91c7-e023-472f-8fce-810757cfd37a",
   "metadata": {},
   "source": [
    "## Get obs at NNPF times\n",
    "- verify, plot, overlay on SPC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb19b8-a026-4f37-9278-a44092dd78ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "obs = spc_fcst.reorder_levels(p.index.names).loc[p.index, [f, \"DN\"]]\n",
    "fcst = p[f]\n",
    "\n",
    "how=\"floor\"\n",
    "ztfs_func = partial(lightning.ztfs, how=how)\n",
    "fcst = fcst.map(ztfs_func)  # round NNPF probs to limited SPC prob levels\n",
    "\n",
    "keep_SPC_fig = len(prodidss) == 1\n",
    "if not keep_SPC_fig:\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 4, nrows * 4))\n",
    "\n",
    "nnpf_bss = {}\n",
    "for prodids in prodidss:\n",
    "    iprodid = (\n",
    "        pd.Series(list(zip(issue, valid_start_hours, valid_end_hours)))\n",
    "        .isin(prodids)\n",
    "        .values\n",
    "    )\n",
    "    print(f\"n={iprodid.sum()/G211.mask.values.sum():.0f} {o_thresh}+{f} {prodids}\", end=\" \")\n",
    "\n",
    "    ihr = spc_fcst.prodid.isin(prodids)\n",
    "    assert ihr.sum()\n",
    "\n",
    "    # Sanity check\n",
    "    # NNPF forecasts are missing when obsvar is missing in twin-h time window.\n",
    "    # However there may be valid obsvar within longer valid time window of SPC forecast.\n",
    "    # get_obs allows for missing obsvar. It just takes the mean of what is\n",
    "    # available and then multiplies it by the total time window length.\n",
    "    # e.g. you get a valid lightning count from\n",
    "    # get_obs(pd.to_datetime(\"20210827T20\"), pd.to_datetime(\"20210828T00\"), obsvar, 2, rptdist)\n",
    "    # but labels is missing for one of the 2-h components of that 4-hour SPC forecast valid time window.\n",
    "    # labels.loc[(\"2021-08-27\", \"2021-08-27 21:00:00\", 12, 48, slice(None))]\n",
    "    missing_nnpf_forecasts = set(spc_fcst[ihr].groupby(\"record\").first().index) - set(\n",
    "        p[iprodid].groupby(\"record\").first().index\n",
    "    )\n",
    "    if len(missing_nnpf_forecasts):\n",
    "        logging.warning(f\"{len(missing_nnpf_forecasts)} missing NNPF forecasts\")\n",
    "        print(missing_nnpf_forecasts)\n",
    "\n",
    "    missing_SPC_forecasts = set(p[iprodid].groupby(\"record\").first().index) - set(\n",
    "        spc_fcst[ihr].groupby(\"record\").first().index\n",
    "    )\n",
    "    if len(missing_SPC_forecasts):\n",
    "        logging.warning(f\"{len(missing_SPC_forecasts)} missing SPC forecasts\")\n",
    "        print(missing_SPC_forecasts)\n",
    "\n",
    "\n",
    "    thisf = fcst[iprodid]\n",
    "    thisf.name = \"NNPF\" + valid_hour_str(prodids)\n",
    "\n",
    "    fig = stat_plots(\n",
    "        obs.loc[iprodid, f],\n",
    "        thisf,\n",
    "        thresh=thresh,\n",
    "        pthresh=pthreshSPC,\n",
    "        o_thresh_roc=o_thresh,\n",
    "        sep=0.15,\n",
    "        n_bins=11,\n",
    "        fig=fig,\n",
    "    )\n",
    "    if len(prodids) == 1:\n",
    "        nnpf_bss[prodids[0]] = brier_skill_score(obs.loc[iprodid,f] >= o_thresh, thisf)        \n",
    "    \n",
    "        # make sure bss is same as before\n",
    "        t = brier_skill_score(obs.loc[iprodid,f] >= o_thresh, obs.loc[iprodid, \"DN\"]/100.)\n",
    "        if spc_bss[prodids[0]] != t:\n",
    "            logging.warning(f\"old spc bss {spc_bss[prodids[0]]} != new {t}\")\n",
    "axes[0,0].set_xlabel(\"forecast probability\") # neat for paper\n",
    "axes[0,0].set_ylabel(\"observed relative frequency\") # neat for paper\n",
    "\n",
    "\n",
    "title = f\"NNPF {o_thresh}+{f}  valid [{valid_start.min()}, {valid_end.max()}]\"\n",
    "plt.suptitle(title, wrap=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "ofile = tmpdir / f\"{how}_NNPF.{o_thresh:03d}+{f}.pdf\"\n",
    "fig.savefig(ofile, dpi=dpi)\n",
    "logging.warning(f\"made {ofile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e896e-75f1-4160-b71f-ed86620508fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8462e-d2f1-47d3-bc08-16b374d334b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df, o_thresh=o_thresh):\n",
    "    labels = df[f] >= o_thresh\n",
    "    nnpf_pred = df[f+\"NNPF\"].map(ztfs_func)\n",
    "    spc_pred = df[\"DN\"]/100.\n",
    "\n",
    "    bs_nnpf = np.mean((nnpf_pred - labels)**2)\n",
    "    bs_spc  = np.mean((spc_pred  - labels)**2)\n",
    "    bss_nnpf = brier_skill_score(labels, nnpf_pred)\n",
    "    bss_spc  = brier_skill_score(labels,  spc_pred)\n",
    "\n",
    "    df = pd.Series(\n",
    "        {\n",
    "            \"bs_nnpf\": bs_nnpf,\n",
    "            \"bs_spc\": bs_spc,\n",
    "            \"bss_nnpf\": bss_nnpf,\n",
    "            \"bss_spc\": bss_spc,\n",
    "            \"base_rate\": labels.mean(),\n",
    "            \"n\": labels.count()/G211.mask.values.sum(),\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bb7b8-e597-49bd-a5b3-71d6b8a02735",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = spc_fcst.join(p, rsuffix=\"NNPF\").dropna() # TODO: remove dropna(). needed to deal with missing NNPF when matching SPC in 2022\n",
    "        \n",
    "def bootstrap(prodid):\n",
    "    ofile = tmpdir / f\"bootstrap.{args.teststart.strftime('%Y%m%d%H')}-{args.testend.strftime('%Y%m%d%H')}.{prodid[0]}.{prodid[1]}.{prodid[2]}.par\"\n",
    "    logging.warning(ofile)\n",
    "    if os.path.exists(ofile):\n",
    "        return pd.read_parquet(ofile)\n",
    "    x = xx[xx[\"prodid\"] == prodid]\n",
    "    idates = pd.to_datetime(x.valid_start).dt.round(freq=\"1D\")\n",
    "    ss = []\n",
    "    for i in tqdm(range(1000)):\n",
    "        resampledDates = sklearn.utils.resample(idates.unique(), replace=True)\n",
    "        vc = pd.Series(resampledDates).value_counts()\n",
    "        xi = pd.concat([x.reindex(x[idates == v].index.repeat(c)) for v, c in vc.items()])\n",
    "        ss.append(stats(xi))\n",
    "    ss = pd.concat(ss, axis=\"columns\").T\n",
    "    ss.to_parquet(ofile)\n",
    "    return ss\n",
    "\n",
    "with Pool(processes=6) as pool:\n",
    "    sss = pool.map(bootstrap, flat_prods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38ea17-8b94-40c4-a39b-96c6afb13463",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.concat(sss, keys=flat_prods, names=[\"issue\",\"start_hr\",\"end_hr\"]).reset_index(level=[\"issue\",\"start_hr\",\"end_hr\"])\n",
    "ss['prodid'] = list(zip(ss.issue, ss.start_hr, ss.end_hr))\n",
    "ss[\"d\"] = ss[\"bss_nnpf\"] - ss[\"bss_spc\"]\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4538c-810b-4f6c-aab3-d45f3089a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, sharey=True, figsize=(9,4))\n",
    "sns.boxplot(data = ss, y=\"d\", hue=\"prodid\", ax=axes[0], legend=False)\n",
    "sns.violinplot(data = ss, y=\"d\", hue=\"prodid\", ax=axes[1])\n",
    "sns.move_legend(axes[1], \"center left\", bbox_to_anchor=(1,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1790a7-6516-4e8b-bfc8-d31b86f90424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence_level=0.95):\n",
    "    if False:\n",
    "        a = 1.0 * np.array(data)\n",
    "        n = len(a)\n",
    "        m, se = np.mean(a), scipy.stats.sem(a)\n",
    "        h = se * scipy.stats.t.ppf((1 + confidence_level) / 2., n-1)\n",
    "        return m, h\n",
    "    else:\n",
    "        qlo = (1-confidence_level)/2 * 100\n",
    "        qhi = 100-qlo\n",
    "        lo, hi = np.percentile(data, [qlo, qhi])\n",
    "        m = data.mean()\n",
    "        return lo, data.mean(), hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, **kwargs):\n",
    "    res =  scipy.stats.bootstrap((data,), np.mean, **kwargs)\n",
    "    ci_l, ci_u = res.confidence_interval\n",
    "    return ci_l, np.mean(res.bootstrap_distribution), ci_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0f34b-af6b-4369-b9f6-34dcb7216ef2",
   "metadata": {},
   "source": [
    "## Compare BSS of SPC and NNPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed57f1-1367-4913-9673-28e64318dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"d\"\n",
    "ss.groupby(\"prodid\")[d].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95155f-f90c-4c06-8ea5-5a59c9b35ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ss.groupby(\"prodid\")[d].apply(mean_confidence_interval, confidence_level=.9)\n",
    "s=pd.DataFrame(s.tolist(), index=s.index, columns=[\"l\", d, \"h\"])\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(9,3))\n",
    "s[d].plot.bar(ax=ax, yerr=(s[d] - s[\"l\"], s[\"h\"] - s[d])).axhline(y=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e1c32-c014-400f-88a2-fd5ef4eb5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ss.groupby(\"prodid\")[d].apply(mean_confidence_interval, confidence_level=0.9)\n",
    "\n",
    "s = pd.DataFrame(\n",
    "    s.tolist(),\n",
    "    index=pd.MultiIndex.from_tuples(s.index, names=[\"issuance (HHMM UTC)\", \"start hour\", \"end hour\"]),\n",
    "    columns=[\"l\", d, \"h\"],\n",
    ")\n",
    "s = (\n",
    "    s.reindex(\n",
    "        [\n",
    "            (\"0600\", 12, 16),\n",
    "            (\"0600\", 16, 20),\n",
    "            (\"0600\", 20, 0),\n",
    "            (\"1300\", 0, 4),\n",
    "            (\"1300\", 16, 20),\n",
    "            (\"1300\", 20, 0),\n",
    "            (\"1700\", 0, 4),\n",
    "            (\"1700\", 4, 12),\n",
    "            (\"1700\", 20, 0),\n",
    "            (\"2100\", 0, 4),\n",
    "            (\"2100\", 4, 12),\n",
    "            (\"0130\", 4, 12),\n",
    "        ]\n",
    "    )\n",
    "    .unstack(level=[\"start hour\", \"end hour\"])\n",
    "    .reindex([\"0130\", \"2100\", \"1700\", \"1300\", \"0600\"])\n",
    ")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e8e4a-28e3-4ee4-9894-65afde240cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = (s[\"l\"] * s[\"h\"] > 0).map(lambda x: \"*\" if x else \"\")\n",
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f421836-2e2d-4aa8-9944-ecd6eb11a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nnpf_bss, \"do you need to rerun with each issue/valid time combination in its own list?\"\n",
    "\n",
    "bss_df = pd.concat(\n",
    "    [pd.Series(spc_bss, name=\"SPC\"), pd.Series(nnpf_bss, name=\"NNPF\")], axis=\"columns\"\n",
    ")\n",
    "bss_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(i, f\"{j:02d}-{k:02d}\") for i, j, k in bss_df.index]\n",
    ")\n",
    "valid_hr_str = \"valid range (hour UTC)\"\n",
    "bss_df.index.names = [\"issuance\", valid_hr_str]\n",
    "bss_df.columns.name = \"product\"\n",
    "bss_df = bss_df.reset_index().melt(\n",
    "    id_vars=[\"issuance\", valid_hr_str], value_name=\"Brier Skill Score\"\n",
    ")\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(9, 3.8), width_ratios=[0.8,1])\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    bss_df,\n",
    "    x=valid_hr_str,\n",
    "    y=\"Brier Skill Score\",\n",
    "    hue=\"product\",\n",
    "    style=\"issuance\",\n",
    "    s=100,\n",
    "    ax=axes[0],\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "sns.move_legend(g, loc=\"upper left\", bbox_to_anchor=(1, 1, 0.1, 0.), fontsize=\"small\")\n",
    "\n",
    "g.set_title(\"(a) Brier Skill Score\", loc=\"left\")\n",
    "g.set_ylim(0.15, 0.37)\n",
    "\n",
    "g = sns.heatmap(\n",
    "    s[d],\n",
    "    ax=axes[1],\n",
    "    annot=s[d].round(decimals=2).astype(str) + sig,\n",
    "    fmt=\"\",\n",
    "    vmin=-0.12,\n",
    "    vmax=0.12,\n",
    "    cmap=sns.diverging_palette(45, 140, s=100, as_cmap=True),\n",
    ")\n",
    "g.set_title(\"(b) BSS differences (NNPF-SPC)\", loc=\"left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "ofile = tmpdir / f\"DNNSPC.{o_thresh}+{f}.bss.{args.teststart.strftime('%Y')}.pdf\"\n",
    "fig.savefig(ofile, dpi=dpi)\n",
    "logging.warning(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88976587-ac9f-4c75-a393-ad3082fc4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "bss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adf561-3c3f-450b-84b4-2c4e89b9f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
